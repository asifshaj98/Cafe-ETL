# Data-engineering-final-project - Group 4

### Contributors:
**Team 4**, Data Engineering DE-LON8 Cohort
<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align="center"><a href="https://www.linkedin.com/in/asif-shaj/"><img src="https://media.licdn.com/dms/image/C4D03AQHfHgSaJiCrDw/profile-displayphoto-shrink_800_800/0/1565716570776?e=1679529600&v=beta&t=xLJDktoglKlkgupyGea6g5zs8iuWahOU-gratHZ2oZo" width="100px;" alt=""/><br /><sub><b>Asif Shajahan</b></sub></a><br /><a href="https://github.com/asifshaj98" title="Documentation">ðŸ“–</a> <sub><b>:medal_military:</b></sub></a></td>
    <td align="center"><a href="https://www.linkedin.com/in/sheikh-osman-a15a29260/"><img src="https://avatars.githubusercontent.com/u/115299269?v=4" width="100px;" alt=""/><br /><sub><b>Sheikh Osman</b></sub></a><br /><a href="https://github.com/IceWindFour" title="Documentation">ðŸ“–</a> <sub><b>:medal_military:</b></sub></a></td>
     <td align="center"><a href="https://www.linkedin.com/in/faaruq-mansaray-4bb07b240/"><img src="https://media.licdn.com/dms/image/D4E03AQHZnxzf7clpEg/profile-displayphoto-shrink_800_800/0/1673812589476?e=1679529600&v=beta&t=-QyywKg9hLL6d__H1VMLET0nXHiMNx3p_SF18awlx9I" width="100px;" alt=""/><br /><sub><b>Faaruq Mansaray</b></sub></a><br /><a href="https://github.com/OmgFaaruq" title="Documentation">ðŸ“–</a> <sub><b>:medal_military:</b></sub></a></td>
     <td align="center"><a href="https://www.linkedin.com/in/daniel-lui-uk/"><img src="https://media.licdn.com/dms/image/C5603AQH3tyNuCiOJwA/profile-displayphoto-shrink_800_800/0/1629199050430?e=1679529600&v=beta&t=tKLZgEsr8iXshu6D9SVmD17t65qXrc9eCaUEIT-M63E" width="100px;" alt=""/><br /><sub><b>Daniel Lui</b></sub></a><br /><a href="https://github.com/danielluimkuk" title="Documentation">ðŸ“–</a> <sub><b>:medal_military:</b></sub></a></td>
  </tr>
</table>


## Overview 

The cafÃ© that we had  previously worked for has seen unprecedented growth and has expanded to hundreds of outlets across the country. The company is experiencing issues with collating and analysing the data produced at each branch because their technical setup is limited. We have been given the task of   providing consultation on what they need to do in order to grow their technical offerings, so that they can continue to accelerate their growth. The company currently has no way of identifying trends, meaning they are potentially losing out on major revenue streams. We therefore will build a fully scalable ETL (Extract, Transform, Load) pipeline to handle large volumes of transaction data for the business. The pipeline will collect all the transaction data generated by each individual cafÃ© and place it in a single location. Data analytics software will be used to create Business Intelligence analytics for the client and Application monitoring software will be used to produce operational metrics, such as system errors, up-time and more.

## Current Setup
Every day, the following occurs for each branch:
-  A CSV file containing data about every transaction they made for that day is generated.
- At 8pm, the data is uploaded to a piece of software installed in the back office computers.
- Daily, weekly or monthly reports for sales figures and other related business metrics are created.
## The Problem
- The software currently being used only generates reports for single branches.
- It is time consuming to collate data on all branches.
- Gathering meaningful data for the company on the whole is difficult, due to the limitations of the software.
- The company currently has no way of identifying trends.
## The Solution
After a thorough analysis and discovery of what the client is looking for, a plan has been fleshed out. We have been tasked with building a fully scalable ETL (Extract, Transform, Load) pipeline to handle large volumes of transaction data for the business. This pipeline will collect all the transaction data generated by each individual cafÃ© and place it in a single location. By being able to easily query the company's data as a whole, the client will drastically increase their ability to identify company-wide trends and insights.
## Requirements
- Each night a CSV for each branch will be uploaded to the cloud.
- The system we have developed will read each file and perform ETL steps.
- Data will be stored in a datawarehouse.
- Data analytics software will be used to create Business Intelligence analytics for the client.
- Application monitoring software will be used to produce operational metrics, such as system errors, up-time and more.
## Ways of Working

The project will consist of fivesprints, where each sprint is a week in length.



## **Development Roadmap**
### Sprint One - Setup basic ETL pipeline (Proof of concept)
- [x] Extraction of data
- [x] Design schema to model data
- [x] Normalise data to 3NF
- [x] Create SQL script to generate
- [x] Setup docker-compose file to create network of containers

### Sprint Two - Move ETL pipeline to the cloud
- [x] Set up database in AWS Redshift
- [x] Set up AWS Lambda to be triggered by and S3 event
- [x] Modify Labda to run ETL
- [x] Configure cloudformation

### Sprint Three - Automate Deployment and Visualise Data
- [x] Grafana Set up
- [x] Visualise Sales Data
- [x] Set Up CI/CD (github actions) Pipeline



## Installation

#### Clone repo
```python
git clone git@github.com:DELON8/group-4-data-engineering-final-project.git
```
```bash
  pip install -r requirements.txt 
```

## Docker for PostgreSQL setup
Began by creating a directoryand then create a docker-compose.yml file within the directory:

```python
mkdir postgres
cd postgres
touch docker-compose.yml
```




```python
version: "3.7"
services:
  db:
    image: postgres
    container_name: postgres
    restart: always
    environment:
      POSTGRES_PASSWORD: "pass"
    ports:
      - 5432:5432
    volumes:
      - data_db:/var/lib/postgresql/data
  adminer:
    image: adminer
    container_name: adminer
    restart: always
    ports:
      - 6969:8080
volumes:
  data_db:
```



## Deployment

To deploy this project run

```bash
  bash command to be given
```


## Lessons Learned

What did you learn while building this project? What challenges did you face and how did you overcome them?

#### General
- ETL
- DataWarehousing
- DataAnalytics/Visualisations/BusinessIntelligence
- Monitoring
- DevelopmentOperations(DevOps)
- Python for developing the appplication
- GitHub for source control
- Jira for project management
- AWS Services such as S3, Lambda, Redshift, Quicksight and Cloudwatch
- Grafana for app monitoring
#### Sprint 1 
- Writing tickets on Jira board, e.g. 3 Amigos, setting value points and acceptance criteria etc.
- Breaking and merging code with git functions, e.g. pull, commit, push, branches, rebase etc.

## Usage/Examples

```javascript
import Component from 'my-project'

function App() {
  return <Component />
}
```


## Running Tests

To run tests, run the following command

```bash
  npm run test
```


## Screenshots

![App Screenshot](https://via.placeholder.com/468x300?text=App+Screenshot+Here)






